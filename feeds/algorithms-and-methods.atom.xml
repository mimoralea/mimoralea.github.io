<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>mimoralea.com</title><link href="http://www.mimoralea.com/" rel="alternate"></link><link href="http://www.mimoralea.com/feeds/algorithms-and-methods.atom.xml" rel="self"></link><id>http://www.mimoralea.com/</id><updated>2015-09-08T13:39:00-05:00</updated><entry><title>Recursion, Memoization and Dynamic Programming</title><link href="http://www.mimoralea.com/recursion-memoization-and-dynamic-programming.html" rel="alternate"></link><published>2015-09-08T13:39:00-05:00</published><author><name>Miguel Morales</name></author><id>tag:www.mimoralea.com,2015-09-08:recursion-memoization-and-dynamic-programming.html</id><summary type="html">&lt;p&gt;I wanted to quickly show the differences between these 3 very related computer science concepts that tend to be&amp;nbsp;mixed.&lt;/p&gt;
&lt;p&gt;For that, I will work on a well know problem, the Fibonacci&amp;nbsp;series.&lt;/p&gt;
&lt;p&gt;The Fibonacci series looks something like: 0, 1, 1, 2, 3, 5, 8, 13, 21 &amp;#8230; and so on. Any person with sufficient analytical skills would quickly notice the&amp;nbsp;pattern.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;f(n) = f(n-1) + f(n-2)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There are many ways someone can tackle this problem, the most intuitive one is&amp;nbsp;recursion.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fib&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Timing this algorithm gives is about 257ms wall&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%time fib(30)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="caps"&gt;CPU&lt;/span&gt; times: user 266 ms, sys: 7.17 ms, total: 273&amp;nbsp;ms&lt;/p&gt;
&lt;p&gt;Wall time: 257&amp;nbsp;ms&lt;/p&gt;
&lt;p&gt;832040&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now, the main problem of this algorithm is that we are computing some of the subproblems more than once. For instance, to compute &lt;code&gt;fib(4)&lt;/code&gt; we would compute &lt;code&gt;fib(3)&lt;/code&gt; and &lt;code&gt;fib(2)&lt;/code&gt;.
However, to compute &lt;code&gt;fib(3)&lt;/code&gt; we also have to compute &lt;code&gt;fib(2)&lt;/code&gt;. Say hello to&amp;nbsp;memoization.&lt;/p&gt;
&lt;p&gt;In memoization we are doing nothing but caching the results of previously computed sub&amp;nbsp;problems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fibm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="n"&gt;fibm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fibm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This algorithm is faster since we are avoiding extra computations, instead we are using a little more&amp;nbsp;memory.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%time fibm(30)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="caps"&gt;CPU&lt;/span&gt; times: user 38 µs, sys: 0 ns, total: 38&amp;nbsp;µs&lt;/p&gt;
&lt;p&gt;Wall time: 27.2&amp;nbsp;µs&lt;/p&gt;
&lt;p&gt;832040&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But the question is, can we do better than this? The use of the array is helpful, but when calculating very large numbers,
or perhaps on memory contraint environments it might not be desirable. This is where Dynamic Programming fits the&amp;nbsp;bill.&lt;/p&gt;
&lt;p&gt;In &lt;span class="caps"&gt;DP&lt;/span&gt; we take a bottom-up approach. Meaning, we solve the next Fibonacci number we can with the information we already&amp;nbsp;have.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fibdp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;prev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;curr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;newf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prev&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;curr&lt;/span&gt;
        &lt;span class="n"&gt;prev&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;curr&lt;/span&gt;
        &lt;span class="n"&gt;curr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;newf&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;curr&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this format, we don&amp;#8217;t need to recurse or keep up with the memory intensive cache dictionary. These, add up to an even better&amp;nbsp;performance.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;%time fibdp(30)&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="caps"&gt;CPU&lt;/span&gt; times: user 10 µs, sys: 0 ns, total: 10&amp;nbsp;µs&lt;/p&gt;
&lt;p&gt;Wall time: 10&amp;nbsp;µs&lt;/p&gt;
&lt;p&gt;832040&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Pretty cool,&amp;nbsp;isn&amp;#8217;t?&lt;/p&gt;
&lt;p&gt;There is a lot of good information around the web to help you understand these methods better. I hope, however, that this post served you&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Keep&amp;nbsp;hacking!&lt;/p&gt;</summary></entry></feed>